dataset,version,metric,mode,high-energy-2b
agieval-gaokao-chinese,774562,accuracy,gen,18.70
agieval-gaokao-english,cb5bc6,accuracy,gen,27.12
agieval-gaokao-geography,2ca56f,accuracy,gen,23.12
agieval-gaokao-history,9c3ae0,accuracy,gen,22.98
agieval-gaokao-biology,277c85,accuracy,gen,21.43
agieval-gaokao-chemistry,d62fd4,accuracy,gen,27.54
agieval-gaokao-mathqa,e2ea74,accuracy,gen,27.92
agieval-logiqa-zh,03474d,accuracy,gen,25.19
agieval-lsat-ar,ed1edf,accuracy,gen,17.39
agieval-lsat-lr,ec6882,accuracy,gen,16.47
agieval-lsat-rc,33077d,accuracy,gen,17.47
agieval-logiqa-en,c6ee60,accuracy,gen,17.82
agieval-sat-math,6c970d,accuracy,gen,28.18
agieval-sat-en,4e3fef,accuracy,gen,21.36
agieval-sat-en-without-passage,4e3fef,accuracy,gen,27.18
agieval-aqua-rat,c090ca,accuracy,gen,17.32
agieval-gaokao-physics,7704e3,accuracy,gen,22.50
agieval-jec-qa-kd,b1e586,accuracy,gen,11.70
agieval-jec-qa-ca,47bc29,accuracy,gen,10.70
agieval-gaokao-mathcloze,b2c3c3,score,gen,0.85
agieval-math,e0c6d9,score,gen,1.40
medqa_en,-,-,-,-
lukaemon_mmlu_college_biology,caec7d,accuracy,gen,23.61
lukaemon_mmlu_college_chemistry,520aa6,accuracy,gen,23.00
lukaemon_mmlu_college_computer_science,99c216,accuracy,gen,19.00
lukaemon_mmlu_college_mathematics,678751,accuracy,gen,27.00
lukaemon_mmlu_college_physics,4f382c,accuracy,gen,28.43
lukaemon_mmlu_electrical_engineering,770ce3,accuracy,gen,22.07
lukaemon_mmlu_astronomy,d3ee01,accuracy,gen,27.63
lukaemon_mmlu_anatomy,72183b,accuracy,gen,23.70
lukaemon_mmlu_abstract_algebra,2db373,accuracy,gen,20.00
lukaemon_mmlu_machine_learning,0283bb,accuracy,gen,31.25
lukaemon_mmlu_clinical_knowledge,cb3218,accuracy,gen,31.32
lukaemon_mmlu_global_facts,ab07b6,accuracy,gen,27.00
lukaemon_mmlu_management,80876d,accuracy,gen,23.30
lukaemon_mmlu_nutrition,4543bd,accuracy,gen,29.74
lukaemon_mmlu_marketing,7394e3,accuracy,gen,26.07
lukaemon_mmlu_professional_accounting,444b7f,accuracy,gen,24.82
lukaemon_mmlu_high_school_geography,0780e6,accuracy,gen,28.28
lukaemon_mmlu_international_law,cf3179,accuracy,gen,26.45
lukaemon_mmlu_moral_scenarios,f6dbe2,accuracy,gen,22.91
lukaemon_mmlu_computer_security,ce7550,accuracy,gen,19.00
lukaemon_mmlu_high_school_microeconomics,04d21a,accuracy,gen,24.79
lukaemon_mmlu_professional_law,5f7e6c,accuracy,gen,25.10
lukaemon_mmlu_medical_genetics,881ef5,accuracy,gen,33.00
lukaemon_mmlu_professional_psychology,221a16,accuracy,gen,25.33
lukaemon_mmlu_jurisprudence,001f24,accuracy,gen,25.00
lukaemon_mmlu_world_religions,232c09,accuracy,gen,27.49
lukaemon_mmlu_philosophy,08042b,accuracy,gen,20.90
lukaemon_mmlu_virology,12e270,accuracy,gen,24.70
lukaemon_mmlu_high_school_chemistry,ae8820,accuracy,gen,28.08
lukaemon_mmlu_public_relations,e7d39b,accuracy,gen,27.27
lukaemon_mmlu_high_school_macroeconomics,a01685,accuracy,gen,26.41
lukaemon_mmlu_human_sexuality,42407c,accuracy,gen,26.72
lukaemon_mmlu_elementary_mathematics,269926,accuracy,gen,23.81
lukaemon_mmlu_high_school_physics,93278f,accuracy,gen,22.52
lukaemon_mmlu_high_school_computer_science,9965a5,accuracy,gen,18.00
lukaemon_mmlu_high_school_european_history,eefc90,accuracy,gen,0.00
lukaemon_mmlu_business_ethics,1dec08,accuracy,gen,24.00
lukaemon_mmlu_moral_disputes,a2173e,accuracy,gen,26.01
lukaemon_mmlu_high_school_statistics,8f3f3a,accuracy,gen,40.74
lukaemon_mmlu_miscellaneous,935647,accuracy,gen,24.78
lukaemon_mmlu_formal_logic,cfcb0c,accuracy,gen,30.16
lukaemon_mmlu_high_school_government_and_politics,3c52f9,accuracy,gen,24.35
lukaemon_mmlu_prehistory,bbb197,accuracy,gen,26.23
lukaemon_mmlu_security_studies,9b1743,accuracy,gen,37.96
lukaemon_mmlu_high_school_biology,37b125,accuracy,gen,29.03
lukaemon_mmlu_logical_fallacies,9cebb0,accuracy,gen,25.77
lukaemon_mmlu_high_school_world_history,048e7e,accuracy,gen,26.58
lukaemon_mmlu_professional_medicine,857144,accuracy,gen,30.51
lukaemon_mmlu_high_school_mathematics,ed4dc0,accuracy,gen,25.56
lukaemon_mmlu_college_medicine,38709e,accuracy,gen,32.37
lukaemon_mmlu_high_school_us_history,8932df,accuracy,gen,0.00
lukaemon_mmlu_sociology,c266a2,accuracy,gen,22.89
lukaemon_mmlu_econometrics,d1134d,accuracy,gen,28.07
lukaemon_mmlu_high_school_psychology,7db114,accuracy,gen,30.09
lukaemon_mmlu_human_aging,82a410,accuracy,gen,14.80
lukaemon_mmlu_us_foreign_policy,528cfe,accuracy,gen,30.00
lukaemon_mmlu_conceptual_physics,63588e,accuracy,gen,25.96
cmmlu-agronomy,4c7f2c,accuracy,gen,24.26
cmmlu-anatomy,ea09bf,accuracy,gen,28.38
cmmlu-ancient_chinese,f7c97f,accuracy,gen,24.39
cmmlu-arts,dd77b8,accuracy,gen,21.25
cmmlu-astronomy,1e49db,accuracy,gen,26.06
cmmlu-business_ethics,dc78cb,accuracy,gen,24.88
cmmlu-chinese_civil_service_exam,1de82c,accuracy,gen,25.00
cmmlu-chinese_driving_rule,b8a42b,accuracy,gen,25.95
cmmlu-chinese_food_culture,2d568a,accuracy,gen,29.41
cmmlu-chinese_foreign_policy,dc2427,accuracy,gen,19.63
cmmlu-chinese_history,4cc7ed,accuracy,gen,23.53
cmmlu-chinese_literature,af3c41,accuracy,gen,26.96
cmmlu-chinese_teacher_qualification,87de11,accuracy,gen,24.58
cmmlu-clinical_knowledge,c55b1d,accuracy,gen,21.94
cmmlu-college_actuarial_science,d3c360,accuracy,gen,24.53
cmmlu-college_education,df8790,accuracy,gen,21.50
cmmlu-college_engineering_hydrology,673f23,accuracy,gen,29.25
cmmlu-college_law,524c3a,accuracy,gen,21.30
cmmlu-college_mathematics,e4ebad,accuracy,gen,25.71
cmmlu-college_medical_statistics,55af35,accuracy,gen,20.75
cmmlu-college_medicine,702f48,accuracy,gen,27.11
cmmlu-computer_science,637007,accuracy,gen,23.04
cmmlu-computer_security,932b6b,accuracy,gen,19.88
cmmlu-conceptual_physics,cfc077,accuracy,gen,20.41
cmmlu-construction_project_management,968a4a,accuracy,gen,28.06
cmmlu-economics,ddaf7c,accuracy,gen,24.53
cmmlu-education,c35963,accuracy,gen,28.22
cmmlu-electrical_engineering,70e98a,accuracy,gen,23.26
cmmlu-elementary_chinese,cbcd6a,accuracy,gen,23.02
cmmlu-elementary_commonsense,a67f37,accuracy,gen,24.75
cmmlu-elementary_information_and_technology,d34d2a,accuracy,gen,19.33
cmmlu-elementary_mathematics,a9d403,accuracy,gen,22.17
cmmlu-ethnology,31955f,accuracy,gen,22.22
cmmlu-food_science,741d8e,accuracy,gen,23.08
cmmlu-genetics,c326f7,accuracy,gen,23.30
cmmlu-global_facts,0a1236,accuracy,gen,21.48
cmmlu-high_school_biology,2be811,accuracy,gen,24.85
cmmlu-high_school_chemistry,d63c05,accuracy,gen,22.73
cmmlu-high_school_geography,5cd489,accuracy,gen,26.27
cmmlu-high_school_mathematics,6b2087,accuracy,gen,30.49
cmmlu-high_school_physics,3df353,accuracy,gen,29.09
cmmlu-high_school_politics,7a88d8,accuracy,gen,17.48
cmmlu-human_sexuality,54ac98,accuracy,gen,22.22
cmmlu-international_law,0f5d40,accuracy,gen,24.86
cmmlu-journalism,a4f6a0,accuracy,gen,25.00
cmmlu-jurisprudence,7843da,accuracy,gen,22.87
cmmlu-legal_and_moral_basis,f906b0,accuracy,gen,25.70
cmmlu-logical,15a71b,accuracy,gen,26.02
cmmlu-machine_learning,bc6ad4,accuracy,gen,31.97
cmmlu-management,e5e8db,accuracy,gen,29.05
cmmlu-marketing,8b4c18,accuracy,gen,22.78
cmmlu-marxist_theory,75eb79,accuracy,gen,20.63
cmmlu-modern_chinese,83a9b7,accuracy,gen,19.83
cmmlu-nutrition,adfff7,accuracy,gen,23.45
cmmlu-philosophy,75e22d,accuracy,gen,23.81
cmmlu-professional_accounting,0edc91,accuracy,gen,27.43
cmmlu-professional_law,d24af5,accuracy,gen,22.75
cmmlu-professional_medicine,134139,accuracy,gen,25.27
cmmlu-professional_psychology,ec920e,accuracy,gen,25.86
cmmlu-public_relations,70ee06,accuracy,gen,22.41
cmmlu-security_study,45f96f,accuracy,gen,20.00
cmmlu-sociology,485285,accuracy,gen,29.65
cmmlu-sports_science,838cfe,accuracy,gen,24.24
cmmlu-traditional_chinese_medicine,3bbf64,accuracy,gen,25.41
cmmlu-virology,8925bf,accuracy,gen,23.67
cmmlu-world_history,57c97c,accuracy,gen,29.19
cmmlu-world_religions,1d0f4b,accuracy,gen,26.25
triviaqa,2121ce,score,gen,20.39
xiezhi-spec_eng,be4d71,accuracy,gen,23.61
xiezhi-spec_chn,db6543,accuracy,gen,24.31
xiezhi-inter_eng,be4d71,accuracy,gen,26.22
xiezhi-inter_chn,db6543,accuracy,gen,25.40
race-middle,9a54b6,accuracy,gen,23.89
race-high,9a54b6,accuracy,gen,24.50
lambada,217e11,accuracy,gen,13.08
IFEval,3321a3,Prompt-level-strict-accuracy,gen,14.60
IFEval,3321a3,Inst-level-strict-accuracy,gen,23.98
IFEval,3321a3,Prompt-level-loose-accuracy,gen,16.27
IFEval,3321a3,Inst-level-loose-accuracy,gen,26.86
winogrande,a9ede5,accuracy,gen,49.72
hellaswag,-,-,-,-
bbh-temporal_sequences,e43931,score,gen,27.20
bbh-disambiguation_qa,d52c61,score,gen,32.00
bbh-date_understanding,a8000b,score,gen,18.00
bbh-tracking_shuffled_objects_three_objects,7964c0,score,gen,22.00
bbh-penguins_in_a_table,fceb27,score,gen,15.07
bbh-geometric_shapes,503c8f,score,gen,0.00
bbh-snarks,42d6ca,score,gen,44.38
bbh-ruin_names,408de8,score,gen,28.80
bbh-tracking_shuffled_objects_seven_objects,7964c0,score,gen,11.60
bbh-tracking_shuffled_objects_five_objects,7964c0,score,gen,14.80
bbh-logical_deduction_three_objects,45ebc5,score,gen,32.40
bbh-hyperbaton,5e5016,score,gen,44.00
bbh-logical_deduction_five_objects,45ebc5,score,gen,18.00
bbh-logical_deduction_seven_objects,45ebc5,score,gen,8.40
bbh-movie_recommendation,cc2fde,score,gen,20.80
bbh-salient_translation_error_detection,5b5f35,score,gen,9.60
bbh-reasoning_about_colored_objects,1cb761,score,gen,15.60
bbh-multistep_arithmetic_two,30f91e,score,gen,0.00
bbh-navigate,1576d9,score,gen,33.20
bbh-dyck_languages,805bea,score,gen,0.00
bbh-word_sorting,9a3f78,score,gen,1.20
bbh-sports_understanding,d3fa77,score,gen,49.20
bbh-boolean_expressions,612c92,score,gen,54.40
bbh-object_counting,781e5c,score,gen,8.40
bbh-formal_fallacies,eada96,score,gen,31.20
bbh-causal_judgement,89eaa4,score,gen,49.20
bbh-web_of_lies,0c0441,score,gen,42.00
math,265cce,accuracy,gen,0.58
TheoremQA,7009de,accuracy,gen,0.88
gsm8k,1d7fe4,accuracy,gen,1.36
openai_humaneval,-,-,-,-
mbpp,1e1056,score,gen,9.20
mbpp,1e1056,pass,gen,46.00
mbpp,1e1056,timeout,gen,1.00
mbpp,1e1056,failed,gen,198.00
mbpp,1e1056,wrong_answer,gen,255.00
